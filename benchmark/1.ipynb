{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attr import define, field\n",
    "from typing import List, Union, ClassVar, Optional, TypeVar\n",
    "import heapq\n",
    "from utils import temporary_file\n",
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "import random\n",
    "from enum import Enum\n",
    "from utils import END_LINE_CHARS\n",
    "from pathlib import Path\n",
    "from sequence import Sequence\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from typing import TypeVar\n",
    "import glob\n",
    "import parse\n",
    "from datasetconfig import DatasetConfig\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "from dataset import Dataset, DatasetType\n",
    "from labels import BinaryLabel\n",
    "from seqentry import SeqEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = TypeVar('U')\n",
    "CHS_ROOT: Path = Path(\"/home_local/vorontsovie/greco-data/release_7a.2021-10-14/full/CHS/Val_intervals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedtoolsexecutor import BedtoolsExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BEDTOOLS_PATH: Path = Path(\"/home_local/dpenzar/bedtools2/bin\")\n",
    "BED_TOOLS_EXECUTOR = BedtoolsExecutor(BEDTOOLS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedentry import BedEntry\n",
    "from beddata import BedData, join_bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BedData.set_defaull_bedtoolsexecutor(BED_TOOLS_EXECUTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genome import Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import List\n",
    "\n",
    "@define\n",
    "class UniqueTagger:\n",
    "    parts: typing.Sequence[str]\n",
    "    vocabularies: dict[str, list[str]] = field(repr=False)\n",
    "    used_tags: set[str] = field(repr=False, factory=set) \n",
    "    max_occupancy: float = 0.5\n",
    "    seed: int = 777\n",
    "    max_size: int = field(init=False)\n",
    "    random_generator: random.Random = field(init=False, repr=False)\n",
    "\n",
    "\n",
    "    DEFAULT_PARTS: ClassVar[typing.Sequence[str]] = (\"adj\", \"adj\", \"nat\", \"ani\")\n",
    "    DEFAULT_PARTS_PATH: ClassVar[dict[str, Path]] = {'adj': Path(\"adjectives.txt\"), 'nat': Path('nations.txt'), 'ani': Path('animals.txt')}\n",
    "\n",
    "    PARTS_FIELD: ClassVar[str]=\"PARTS\"\n",
    "    VOC_FIELD: ClassVar[str] = \"VOC\"\n",
    "    TAGS_FIELD: ClassVar[str] = \"USED_TAGS\"\n",
    "    PARTS_SEP: ClassVar[str] = '-'\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.max_size = self._calc_maxsize()\n",
    "        self.random_generator = random.Random(self.seed)\n",
    "\n",
    "    @classmethod\n",
    "    def make(cls, parts: typing.Sequence[str], parts_path: dict[str, Path]):\n",
    "        dt = {}\n",
    "        for part in parts:\n",
    "            with parts_path[part].open() as inp:\n",
    "                voc =  [line.rstrip(END_LINE_CHARS) for line in inp]\n",
    "                dt[part] = voc\n",
    "        return cls(parts, dt)\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls):\n",
    "        return cls.make(cls.DEFAULT_PARTS, cls.DEFAULT_PARTS_PATH)        \n",
    "\n",
    "    def write(self, path: Union[Path, str]):\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open('w') as store:\n",
    "            dt = {self.PARTS_FIELD: self.parts,\n",
    "                  self.VOC_FIELD: self.vocabularies,\n",
    "                  self.TAGS_FIELD: list(self.used_tags)}\n",
    "            json.dump(dt, store, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Union[Path, str]) -> 'UniqueTagger':\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open() as inp:\n",
    "            dt = json.load(inp)\n",
    "        parts = dt[cls.PARTS_FIELD]\n",
    "        if not isinstance(parts, list):\n",
    "            raise Exception('Wrong format')\n",
    "        vocs = dt[cls.VOC_FIELD]\n",
    "        if not isinstance(vocs, dict):\n",
    "            raise Exception('Wrong format')\n",
    "        tags = dt[cls.TAGS_FIELD]\n",
    "        if not isinstance(tags, list):\n",
    "            raise Exception('Wrong format')\n",
    "\n",
    "        return cls(tuple(parts), vocs, set(tags)) \n",
    "    \n",
    "    def _non_unique_tag(self) -> str:\n",
    "        tag_prts = [self.random_generator.choice(self.vocabularies[p]) for p in self.parts]\n",
    "        tag = self.PARTS_SEP.join(tag_prts)\n",
    "        return tag\n",
    "    \n",
    "    def tag(self) -> str:\n",
    "        if len(self.used_tags) >= self.max_size:\n",
    "            raise Exception(\"Max size for fast random generation reached\")\n",
    "        while (tag := self._non_unique_tag()) in self.used_tags:\n",
    "            pass\n",
    "        self.used_tags.add(tag)\n",
    "        return tag\n",
    "    \n",
    "    def _calc_maxsize(self) -> int:\n",
    "        cnt = 1\n",
    "        for p in self.parts:\n",
    "            cnt *= len(self.vocabularies[p])\n",
    "        return int(cnt * self.max_occupancy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "from contextlib import contextmanager\n",
    "#https://gist.github.com/tylerneylon\n",
    "class RWLock(object):\n",
    "    \"\"\" RWLock class; this is meant to allow an object to be read from by\n",
    "        multiple threads, but only written to by a single thread at a time. See:\n",
    "        https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock\n",
    "        Usage:\n",
    "            my_obj_rwlock = RWLock()\n",
    "            # When reading from my_obj:\n",
    "            with my_obj_rwlock.r_locked():\n",
    "                do_read_only_things_with(my_obj)\n",
    "            # When writing to my_obj:\n",
    "            with my_obj_rwlock.w_locked():\n",
    "                mutate(my_obj)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.w_lock = Lock()\n",
    "        self.num_r_lock = Lock()\n",
    "        self.num_r = 0\n",
    "\n",
    "    def r_acquire(self):\n",
    "        self.num_r_lock.acquire()\n",
    "        self.num_r += 1\n",
    "        if self.num_r == 1:\n",
    "            self.w_lock.acquire()\n",
    "        self.num_r_lock.release()\n",
    "\n",
    "    def r_release(self):\n",
    "        assert self.num_r > 0\n",
    "        self.num_r_lock.acquire()\n",
    "        self.num_r -= 1\n",
    "        if self.num_r == 0:\n",
    "            self.w_lock.release()\n",
    "        self.num_r_lock.release()\n",
    "\n",
    "    @contextmanager\n",
    "    def r_locked(self):\n",
    "        try:\n",
    "            self.r_acquire()\n",
    "            yield\n",
    "        finally:\n",
    "            self.r_release()\n",
    "\n",
    "    def w_acquire(self):\n",
    "        self.w_lock.acquire()\n",
    "\n",
    "    def w_release(self):\n",
    "        self.w_lock.release()\n",
    "\n",
    "    @contextmanager\n",
    "    def w_locked(self):\n",
    "        try:\n",
    "            self.w_acquire()\n",
    "            yield\n",
    "        finally:\n",
    "            self.w_release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "\n",
    "@define\n",
    "class SeqDB:\n",
    "    seq2tag: dict[str, str] = field(factory=dict)\n",
    "    tag2seq: dict[str, str] = field(factory=dict)\n",
    "    tagger: UniqueTagger = field(factory=UniqueTagger.default)\n",
    "    lock: RWLock = RWLock() \n",
    "    \n",
    "    def add(self, seq: Union[Sequence, str]) -> str:\n",
    "        if isinstance(seq, Sequence):\n",
    "            seq = seq.seq\n",
    "        tag = self.get_tag(seq)\n",
    "        if tag is None:\n",
    "            with self.lock.w_locked(): \n",
    "                tag = self.tagger.tag()\n",
    "                self.seq2tag[seq] = tag\n",
    "                self.tag2seq[tag] = seq\n",
    "        return tag\n",
    "\n",
    "    def get_tag(self, seq: Union[Sequence, str]) -> Optional[str]:\n",
    "        if isinstance(seq, Sequence):\n",
    "            seq = seq.seq\n",
    "        with self.lock.r_locked():\n",
    "            tag = self.seq2tag.get(seq)\n",
    "        return tag\n",
    "\n",
    "    def get_seq(self, tag: str) -> Sequence:\n",
    "        with self.lock.r_locked():\n",
    "            seq = self.tag2seq[tag]\n",
    "        return Sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "from experiment import ExperimentType\n",
    "from abc import abstractproperty, abstractmethod, ABCMeta\n",
    "from math import ceil\n",
    "\n",
    "class SubProtocol(metaclass=ABCMeta):\n",
    "\n",
    "    @abstractproperty\n",
    "    def data_type(self) -> ExperimentType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self, cfgs: List[DatasetConfig]):\n",
    "        '''\n",
    "        prepare data given cgfs files of all datasets of specified datatype\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, cfg: Union[List[DatasetConfig], DatasetConfig]) -> 'Dataset':\n",
    "        '''\n",
    "        create dataset from cfg file using it's data and data aquired during\n",
    "        preprocess stage. Data from other cfg should not be used \n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exceptions import WrongDatasetTypeException\n",
    "from typing import Iterator\n",
    "from utils import undict\n",
    "\n",
    "\n",
    "@define\n",
    "class UDataset(metaclass=ABCMeta):\n",
    "    name: str\n",
    "    type: DatasetType\n",
    "    tf_name: str\n",
    "    entries: Optional[List[SeqEntry]] = field(repr=False)\n",
    "    metainfo: dict\n",
    "    disk_path: Path  \n",
    "\n",
    "    SEQUENCE_FIELD: ClassVar[str] = \"sequence\"\n",
    "    LABEL_FIELD: ClassVar[str] = \"label\"\n",
    "    TAG_FIELD: ClassVar[str] = \"tag\"\n",
    "    NO_INFO_VALUE: ClassVar[str] = \"NoInfo\"\n",
    "\n",
    "\n",
    "    def closed(self) -> bool:\n",
    "        return self.entries is None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.closed():\n",
    "            raise Exception(\"Operation on closed dataset\")\n",
    "        return len(self.entries) # type: ignore\n",
    "\n",
    "    def __iter__(self) -> Iterator[SeqEntry]:\n",
    "        if self.closed():\n",
    "            raise Exception(\"Operation on closed dataset\")\n",
    "        return iter(self.entries) # type: ignore\n",
    "\n",
    "    def __getitem__(self, ind: int) -> SeqEntry:\n",
    "        if self.closed():\n",
    "            raise Exception(\"Operation on closed dataset\")\n",
    "        return self.entries[ind] # type: ignore\n",
    "    \n",
    "    def get_fields(self) -> list[str]:\n",
    "        if self.type is DatasetType.TRAIN:\n",
    "            return self.get_train_fields()\n",
    "        if self.type is DatasetType.TEST:\n",
    "            return self.get_test_fields()\n",
    "        if self.type is DatasetType.VALIDATION:\n",
    "            return self.get_valid_fields()\n",
    "        if self.type is DatasetType.FULL:\n",
    "            return self.get_full_fields()\n",
    "        raise WrongDatasetTypeException(f\"{self.type}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_fields(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_test_fields(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_valid_fields(self):\n",
    "        return self.get_train_fields()\n",
    "    \n",
    "    def get_full_fields(self):\n",
    "        return self.get_train_fields()\n",
    "\n",
    "    def get(self, key, default):\n",
    "        try:\n",
    "            val = getattr(self, key)\n",
    "        except AttributeError:\n",
    "            val = self.metainfo.get(key, default)\n",
    "        return val\n",
    "    \n",
    "    def to_tsv(self,\n",
    "               path: Path,\n",
    "               ds_fields: Optional[List[str]] = None):\n",
    "        fields = self.get_fields()\n",
    "        \n",
    "        if ds_fields:\n",
    "           ds_values = [self.get(fl, self.NO_INFO_VALUE) for fl in ds_fields]\n",
    "\n",
    "        with open(path, \"w\") as out:\n",
    "            if ds_fields:\n",
    "                header = \"\\t\".join(fields + ds_fields)\n",
    "            else:\n",
    "                header = \"\\t\".join(fields)\n",
    "            print(header, file=out)\n",
    "            for en in self.entries:\n",
    "                values = [en.get(fld, self.NO_INFO_VALUE) for fld in fields]  \n",
    "                if ds_fields:\n",
    "                    values.extend(ds_values) # type: ignore\n",
    "                print(\"\\t\".join(values), file=out)\n",
    "                    \n",
    "    def to_json(self, path, mode: Optional[DatasetType] = DatasetType.TEST):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_canonical_format(self, path, mode: Optional[DatasetType] = DatasetType.TEST):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def split(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Union[Path, str]) -> 'Dataset':\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " name: str\n",
    "    type: DatasetType\n",
    "    tf_name: str\n",
    "    entries: Optional[List[SeqEntry]] = field(repr=False)\n",
    "    metainfo: dict\n",
    "    disk_path: Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChIPSeqDataset(Dataset):\n",
    "    def get_train_fields(self) -> list[str]:\n",
    "        return [self.TAG_FIELD, self.SEQUENCE_FIELD,  self.LABEL_FIELD]\n",
    "    \n",
    "    def get_test_fields(self):\n",
    "         return [self.TAG_FIELD, self.SEQUENCE_FIELD]\n",
    "   \n",
    "    def to_canonical_format(self, path):\n",
    "        return self.to_tsv(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Union[Path, str], tp: DatasetType, fmt: str) -> 'ChIPSeqDataset':\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        # TODO: fix saving dataset (storing name, tf_name and metainfo in file)\n",
    "        # TODO: add other fmts, maybe using class abstraction\n",
    "        if fmt != \".tsv\":\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        self = cls(name=\"\", tf_name=\"\", entries=[], type=tp, metainfo={})\n",
    "        fieldsname = self.get_fields()\n",
    "        with path.open() as inp:\n",
    "            _ = inp.readline() # header\n",
    "            for line in inp:\n",
    "                fields = line.split()\n",
    "                dt = dict(zip(fieldsname, fields))\n",
    "                entry = SeqEntry(**dt)\n",
    "                self.entries.append(entry)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativesSampler(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def sample(self, prot: 'ChIPSeqIrisProtocol', cfg: DatasetConfig) -> BedData:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# TODO: move sampling functionality and args directly to samplers \n",
    "class ForeignPeakSampler(NegativesSampler):\n",
    "    def sample(self, prot: 'ChIPSeqIrisProtocol', cfg: DatasetConfig) -> BedData:\n",
    "        return prot.get_foreign_peaks(cfg)\n",
    "\n",
    "\n",
    "class ShadesSampler(NegativesSampler):\n",
    "    def sample(self, prot: 'ChIPSeqIrisProtocol', cfg: DatasetConfig) -> BedData:\n",
    "        return prot.get_shades(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define\n",
    "class ChIPSeqIrisProtocol(SubProtocol):   \n",
    "    mx_dist: int\n",
    "    peak_size: int\n",
    "    shades_per_peak: int\n",
    "    negative_tf_ratio: int \n",
    "    genome: Genome = field(repr=False)\n",
    "    root: Path\n",
    "    db: SeqDB = field(factory=SeqDB)\n",
    "    \n",
    "    TF_MERGE_DIR_NAME: ClassVar[str] = \"TF_MERGE\"\n",
    "    TF_FOREIGN_DIR_NAME: ClassVar[str] = \"TF_FOREIGN\"\n",
    "    REAL_PEAKS_FILE_NAME: ClassVar[str] = \"real.bed\"\n",
    "    ALL_PEAKS_FILE_NAME: ClassVar[str] = \"all.bed\"\n",
    "\n",
    "    MX_DIST_FIELD: ClassVar[str] = \"mx_dist\"\n",
    "    PEAK_SIZE_FIELD: ClassVar[str] = \"peak_size\"\n",
    "    SHADES_PER_PEAK_FIELD: ClassVar[str] = \"shades_per_peak\"\n",
    "    NEGATIVE_TF_RATIO_FIELD: ClassVar[str] = \"negative_ratio\"\n",
    "    ROOT_FIELD: ClassVar[str] = \"root_dir\"\n",
    "    GENOME_FIELD: ClassVar[str] = \"genome\"\n",
    "\n",
    "    DEFAULT_ROOT = Path(\".ChIPSeq\")\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> ExperimentType:\n",
    "        return ExperimentType.ChIPSeq\n",
    "\n",
    "    @property\n",
    "    def tf_merge_dir(self) -> Path:\n",
    "        return self.root / self.TF_MERGE_DIR_NAME\n",
    "\n",
    "    @property\n",
    "    def tf_foreign_dir(self) -> Path:\n",
    "        return self.root / self.TF_FOREIGN_DIR_NAME\n",
    "\n",
    "    @property\n",
    "    def real_peaks_path(self) -> Path:\n",
    "        return self.root / self.REAL_PEAKS_FILE_NAME\n",
    "   \n",
    "    def mkdirs(self):\n",
    "        self.root.mkdir(exist_ok=True, parents=True)\n",
    "        self.tf_merge_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.tf_foreign_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        if self.peak_size % 2 != 1:\n",
    "            raise Exception(f\"Peak size must be odd: {self.peak_size}\")\n",
    "        self.mkdirs()\n",
    "\n",
    "    def tf_merge_path(self, tf_name:str) -> Path:\n",
    "        return self.tf_merge_dir / f\"{tf_name}.bed\"\n",
    "\n",
    "    def tf_foreign_path(self, tf_name:str) -> Path:\n",
    "        return self.tf_foreign_dir / f\"{tf_name}.bed\"\n",
    "\n",
    "    def merge_by_tf(self, cfgs: list[DatasetConfig], real_peaks: BedData):\n",
    "        cfgs.sort(key=lambda x : x.tf)\n",
    "        for tf_name, tf_records in groupby(cfgs, key=lambda x : x.tf):\n",
    "            beds = [BedData.from_file(x.path, header=True) for x in tf_records]\n",
    "            jnd = join_bed(beds)\n",
    "            mrg = jnd.merge()\n",
    "            mrg_path = self.tf_merge_path(tf_name)\n",
    "            mrg.write(mrg_path, write_peak=False)\n",
    "\n",
    "            foreign = real_peaks.subtract(mrg, full=True)\n",
    "            foreign_path = self.tf_foreign_path(tf_name)\n",
    "            foreign.write(foreign_path, write_peak=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def map_peaks(bed: BedData) -> dict[int, BedData]:\n",
    "        dt = defaultdict(BedData)\n",
    "        for e in bed:\n",
    "            dt[e.peak].append(BedEntry(e.chr, e.start, e.end))\n",
    "        return dict(dt)\n",
    "\n",
    "    def sample_shades(self, ds: BedData, tf: BedData) -> BedData:\n",
    "        flanks = ds.flank(self.genome, self.mx_dist + self.peak_size // 2)\n",
    "        sbstr = flanks.subtract(tf, full=False)\n",
    "        dt = self.map_peaks(sbstr)\n",
    "        for peak, sgmnts in dt.items():\n",
    "            sgmnts = sgmnts.apply(lambda s: s.truncate(self.peak_size // 2)).filter(lambda s: len(s) > 0)\n",
    "            dt[peak] = sgmnts\n",
    "        \n",
    "        entries = []\n",
    "        for peak, sgmnts in dt.items():\n",
    "            smpl = sgmnts.sample_shades(seg_size=self.peak_size, k=self.shades_per_peak)\n",
    "            entries.extend(smpl)\n",
    "        return BedData(entries)\n",
    "\n",
    "    def get_shades(self, cfg: DatasetConfig) -> BedData:\n",
    "        ds = BedData.from_file(cfg.path, header=True)\n",
    "        tf_file = self.tf_merge_path(cfg.tf)\n",
    "        tf = BedData.from_file(tf_file, header=True)\n",
    "        return self.sample_shades(ds, tf)\n",
    "    \n",
    "    def get_foreign_peaks(self, cfg: DatasetConfig) -> BedData:\n",
    "        ds = BedData.from_file(cfg.path, header=True)\n",
    "        foreign_path = self.tf_foreign_path(cfg.tf)\n",
    "        foreign = BedData.from_file(foreign_path)\n",
    "        size = len(ds) * self.negative_tf_ratio\n",
    "        return foreign.sample(size)\n",
    "    \n",
    "    def make_real_peaks(self, cfgs: List[DatasetConfig]) -> BedData:\n",
    "        beds = [BedData.from_file(x.path, header=True) for x in configs]\n",
    "        real_bed = join_bed(beds)\n",
    "        real_bed.write(self.real_peaks_path)\n",
    "        return real_bed\n",
    "\n",
    "    def preprocess(self, cfgs: List[DatasetConfig]) -> None:\n",
    "        real_bed = self.make_real_peaks(cfgs)\n",
    "        self.merge_by_tf(cfgs, real_bed)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dt(cls, dt: dict) -> 'ChIPSeqIrisProtocol':\n",
    "        mx_dist=int(dt[cls.MX_DIST_FIELD])\n",
    "        peak_size = int(dt[cls.PEAK_SIZE_FIELD])\n",
    "        shades_per_peak = int(dt[cls.SHADES_PER_PEAK_FIELD])\n",
    "        negative_tf_ratio = int(dt[cls.NEGATIVE_TF_RATIO_FIELD])\n",
    "        genome = Genome.from_dir(dt[cls.GENOME_FIELD])\n",
    "\n",
    "        root  = dt.get(cls.ROOT_FIELD, cls.DEFAULT_ROOT)\n",
    "        root = Path(root)\n",
    "    \n",
    "        return cls(mx_dist=mx_dist,\n",
    "                   peak_size=peak_size,\n",
    "                   shades_per_peak=shades_per_peak,\n",
    "                   negative_tf_ratio=negative_tf_ratio,\n",
    "                   genome=genome,\n",
    "                   root=root)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, path: Union[Path, str]) -> 'ChIPSeqIrisProtocol':\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open() as inp:\n",
    "            dt = json.load(inp)\n",
    "        lower_dt = {key.lower() : value for key, value in dt.items() }\n",
    "        return cls.from_dt(lower_dt)\n",
    "\n",
    "    def get_positives(self, cfg: DatasetConfig) -> list[SeqEntry]:\n",
    "        bed = BedData.from_file(cfg.path, header=True)\n",
    "        seqs = self.genome.cut(bed)\n",
    "        entries = [SeqEntry(s, \n",
    "                        BinaryLabel.POSITIVE,\n",
    "                        self.db.add(s), \n",
    "                        {}) for s in seqs]\n",
    "        return entries\n",
    "\n",
    "    def get_negatives(self, cfg: DatasetConfig, sampler: NegativesSampler) -> List[SeqEntry]:\n",
    "        bed = sampler.sample(self, cfg)\n",
    "        seqs = self.genome.cut(bed)\n",
    "        entries = [SeqEntry(s, \n",
    "                        BinaryLabel.NEGATIVE,\n",
    "                        self.db.add(s), \n",
    "                        {}) for s in seqs]\n",
    "        return entries\n",
    "\n",
    "    def prepare_dataset(self, cfg: DatasetConfig, sampler: NegativesSampler, name: str):\n",
    "        pos = self.get_positives(cfg)\n",
    "        neg = self.get_negatives(cfg, sampler)\n",
    "        return ChIPSeqDataset(name=name, \n",
    "                            tf_name=cfg.tf, \n",
    "                            type=cfg.ds_type, \n",
    "                            entries=pos+neg, \n",
    "                            metainfo=cfg.metainfo)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_shades_ds_name(cfg: DatasetConfig) -> str:\n",
    "        return f\"{cfg.name}_shades\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_foreign_ds_name(cfg: DatasetConfig) -> str:\n",
    "        return f\"{cfg.name}_foreign\"\n",
    "\n",
    "    def process(self, cfg: DatasetConfig) -> list[Dataset]:\n",
    "        shades_ds = self.prepare_dataset(cfg,\n",
    "                                         sampler=ShadesSampler(),\n",
    "                                         name=self.get_shades_ds_name(cfg))\n",
    "        foreign_ds = self.prepare_dataset(cfg,\n",
    "                                          sampler=ForeignPeakSampler(),\n",
    "                                          name=self.get_foreign_ds_name(cfg))\n",
    "        return [shades_ds, foreign_ds]\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakfls2configs(files: list[Path], \n",
    "                    name_parser= parse.compile(\"{tf_name}.{fl}@{exp_type}@{name}@Peaks.{unique_tag}.{dt_type}.peaks\")) -> list[DatasetConfig]:\n",
    "    all_records = []\n",
    "    for val_fl in files:\n",
    "        val_fl = val_fl.absolute()\n",
    "        data = name_parser.parse(val_fl.name)\n",
    "        if data is None or isinstance(data, parse.Match):\n",
    "            raise Exception(\"Wrong peakfile name format\")\n",
    "        dt = {\n",
    "            \"name\": data[\"name\"],\n",
    "            \"exp_type\": \"ChIPSeq\",\n",
    "            \"tf\": data['tf_name'],\n",
    "            \"ds_type\": data['dt_type'],\n",
    "            \"path\": val_fl,\n",
    "            \"curation_status\": \"accepted\",\n",
    "            \"protocol\": \"iris\",\n",
    "            \"metainfo\": {}\n",
    "        }\n",
    "        all_records.append(DatasetConfig.from_dict(dt))\n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = [Path(x) for x in glob.glob(str(CHS_ROOT / '*.peaks'))]\n",
    "configs = peakfls2configs(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_CHIPSEQ_CONFIG = {\n",
    "    \"mx_dist\": 300,\n",
    "    \"peak_size\": 301,\n",
    "    \"shades_per_peak\": 1,\n",
    "    \"negative_ratio\": 100,\n",
    "    \"genome\": Path(\"/home_local/dpenzar/hg38\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = ChIPSeqIrisProtocol.from_dt(IRIS_CHIPSEQ_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#protocol.preprocess(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lst = protocol.process(configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_DIR = Path('.ChIPSeqDatasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pwmeval import PWMEvalPFMPredictor, PWMEvalPWMPredictor\n",
    "from _benchmark import ModelEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_BENCHMARK_CONFIG = Path(\"/home_local/dpenzar/test_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(OLD_BENCHMARK_CONFIG) as inp:\n",
    "    dt = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['datasets'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['scorers'] = dt['scorers'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_CONFIG = Path(\"/home_local/dpenzar/chipseq_config.json\")\n",
    "with BENCHMARK_CONFIG.open('w') as out:\n",
    "    json.dump(dt, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkconfig import BenchmarkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = BenchmarkConfig.from_json(BENCHMARK_CONFIG).make_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = Path(\"/home_local/dpenzar/models.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6664\n"
     ]
    }
   ],
   "source": [
    "with open(MODELS_PATH, \"r\") as inp:\n",
    "    models = json.load(inp)\n",
    "print(len(models))\n",
    "for mcfg in models:\n",
    "    tf_name = mcfg['tf']\n",
    "    name = mcfg['name']\n",
    "    path = mcfg[\"path\"]\n",
    "    bench.add_pfm(tf_name, name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add add_dataset method to benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ChIPSeqDataset.load('.ChIPSeqDatasets/THC_0258_foreign.bed', tp=DatasetType.TRAIN, fmt='.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_PROCS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(max_workers=AVAILABLE_PROCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChIPSeqLOGS = Path(\"chipseq_logs\")\n",
    "ChIPSeqLOGS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_DIR = Path(\".ChIPSeqDatasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset SI0159\n",
      "Submited 0 futures\n",
      "Submited 0 futures\n",
      "Dataset THC_0074\n",
      "Submited 0 futures\n",
      "Submited 0 futures\n",
      "Dataset THC_0246\n",
      "Submited 1 futures\n",
      "Finished for CHAMP1.FL@CHS@silly-champagne-burmese@HughesLab.Homer@Motif1.ppm_sumscore@THC_0246_shades\n",
      "Submited 1 futures\n",
      "Finished for CHAMP1.FL@CHS@silly-champagne-burmese@HughesLab.Homer@Motif1.ppm_sumscore@THC_0246_foreign\n",
      "Dataset THC_0462.Rep-MICHELLE_0314\n",
      "Submited 1 futures\n",
      "Finished for MKX.FL@CHS@droopy-tomato-affenpinscher@HughesLab.Homer@Motif1.ppm_sumscore@THC_0462.Rep-MICHELLE_0314_shades\n",
      "Submited 1 futures\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=43'>44</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSubmited \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(futures)\u001b[39m}\u001b[39;00m\u001b[39m futures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m ft \u001b[39min\u001b[39;00m as_completed(futures):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=46'>47</a>\u001b[0m     name \u001b[39m=\u001b[39m futures[ft]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=47'>48</a>\u001b[0m     path \u001b[39m=\u001b[39m ChIPSeqLOGS \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=239'>240</a>\u001b[0m     \u001b[39mif\u001b[39;00m wait_timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=240'>241</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=241'>242</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m (of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) futures unfinished\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=242'>243</a>\u001b[0m                 \u001b[39mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=244'>245</a>\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(wait_timeout)\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=246'>247</a>\u001b[0m \u001b[39mwith\u001b[39;00m waiter\u001b[39m.\u001b[39mlock:\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=247'>248</a>\u001b[0m     finished \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=571'>572</a>\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=572'>573</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=573'>574</a>\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=574'>575</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=309'>310</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=310'>311</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=311'>312</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=312'>313</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=313'>314</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cfg in configs:\n",
    "    print(f\"Dataset {cfg.name}\")\n",
    "    ds_path = DS_DIR/f\"{protocol.get_shades_ds_name(cfg)}.bed\"\n",
    "    ds = ChIPSeqDataset.load(ds_path, tp=DatasetType.TRAIN, fmt='.tsv')\n",
    "    ds.name = protocol.get_shades_ds_name(cfg)\n",
    "    ds.tf_name = cfg.tf\n",
    "    \n",
    "    futures = {}\n",
    "    for model in bench.models:\n",
    "        #if not (isinstance(model, Model) or ds.tf_name is None or model.tfs is None or ds.tf_name in model.tfs):\n",
    "        if not ds.tf_name in model.tfs:\n",
    "            continue\n",
    "        ft = executor.submit(bench.score_model_on_ds, model, ds)\n",
    "        futures[ft] = f\"{model.name}@{ds.name}\"\n",
    "\n",
    "    print(f\"Submited {len(futures)} futures\")\n",
    "\n",
    "    for ft in as_completed(futures):\n",
    "        name = futures[ft]\n",
    "        path = ChIPSeqLOGS / f\"{name}.json\"\n",
    "        try:\n",
    "            scores = ft.result()\n",
    "            with path.open(\"w\") as outp:\n",
    "                json.dump(scores, outp)\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "            print(f\"Error for {name}\")\n",
    "        else:\n",
    "            print(f\"Finished for {name}\")\n",
    "\n",
    "    \n",
    "\n",
    "    ds_path = DS_DIR/f\"{protocol.get_foreign_ds_name(cfg)}.bed\"\n",
    "    ds = ChIPSeqDataset.load(ds_path, tp=DatasetType.TRAIN, fmt='.tsv')\n",
    "    ds.name = protocol.get_foreign_ds_name(cfg)\n",
    "    ds.tf_name = cfg.tf\n",
    "\n",
    "    futures = {}\n",
    "    for model in bench.models:\n",
    "        if not (isinstance(model, Model) or ds.tf_name is None or model.tfs is None or ds.tf_name in model.tfs):\n",
    "            continue\n",
    "        ft = executor.submit(bench.score_model_on_ds, model, ds)\n",
    "        futures[ft] = f\"{model.name}@{ds.name}\"\n",
    "\n",
    "    print(f\"Submited {len(futures)} futures\")\n",
    "    for ft in as_completed(futures):\n",
    "        name = futures[ft]\n",
    "        path = ChIPSeqLOGS / f\"{name}.json\"\n",
    "\n",
    "        try:\n",
    "            scores = ft.result()\n",
    "            with path.open(\"w\") as outp:\n",
    "                json.dump(scores, outp)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error for {name}\")\n",
    "            print(exc)\n",
    "        else:\n",
    "            print(f\"Finished for {name}\")\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3363"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m \u001b[39msum\u001b[39m([e\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m ds_lst[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mentries]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(ds_lst[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mentries)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_lst' is not defined"
     ]
    }
   ],
   "source": [
    "sum([e.label.value for e in ds_lst[0].entries]) / len(ds_lst[0].entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= BedData.from_file(configs[0].path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169882"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pwmeval import PWMEvalPWMPredictor\n",
    "\n",
    "pfm_path = Path(\"/home_local/dpenzar/ibis-challenge/benchmark/example.pwm\")\n",
    "model = PWMEvalPWMPredictor.from_pfm(pfm_path, Path(\"/home_local/dpenzar/PWMEval/pwm_scoring\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.fasta  COPYING  Makefile     pwm_scoring.c  README.md   seqshuffle.c\n",
      "a.txt\t log.txt  pwm_scoring  README\t      seqshuffle\n"
     ]
    }
   ],
   "source": [
    "!ls /home_local/dpenzar/PWMEval/pwm_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = prepare_dataset(protocol, configs[0], sampler=ForeignPeakSampler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChIPSeqDataset(name='SI0159', type=<DatasetType.VALIDATION: 'val'>, tf_name='ZNF454', metainfo={})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol.preprocess(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bed = BedData.from_file('.ChIPSEQ_DB/total.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = BedData()\n",
    "for entry in total_bed:\n",
    "    if len(entry) != PEAK_SIZE: \n",
    "        if entry.peak is not None:\n",
    "            entry = BedEntry.from_center(entry.chr, entry.peak, radius=PEAK_SIZE//2)\n",
    "        else:\n",
    "            entry = BedEntry.from_center(entry.chr, (entry.start + entry.end) // 2, radius=PEAK_SIZE//2)\n",
    "    total_dataset.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_seqs = genome.cut(total_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a6b89ce9ffc256b1b7590cf8df7eee41f38232dcd839c1409bba0f1e7ed7c9d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
